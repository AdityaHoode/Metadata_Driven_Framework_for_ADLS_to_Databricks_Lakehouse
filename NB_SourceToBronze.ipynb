{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda617f2-4adb-41e8-8cbd-49daf3759f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"JobName\", \"\")\n",
    "dbutils.widgets.text(\"JobRunID\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb56a4c5-6901-4ee0-939a-c7291a0ea784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_name = dbutils.widgets.get(\"JobName\")\n",
    "job_run_id = dbutils.widgets.get(\"JobRunID\")\n",
    "\n",
    "manual_override = False\n",
    "if not manual_override:\n",
    "    mappings = dbutils.widgets.get(\"mappings\")\n",
    "else:\n",
    "    mappings = {\n",
    "        \"RootFolder\": \"SalesDataLandingZone\",\n",
    "        \"TargetCatalog\": \"mdf2\",\n",
    "        \"TargetSchema\": \"bronze\",\n",
    "        \"TargetTable\": \"Sales\",\n",
    "        \"FileSearchTerm\": None,\n",
    "        \"BaseUrl\": \"abfss://mdf2@stgacct14022025.dfs.core.windows.net\",\n",
    "        \"WatermarkColumn\": \"Timestamp\",\n",
    "        \"LPWatermarkValue\": '1970-01-01T00:00:00.000+00:00',\n",
    "        \"EnableFlag\": 1,\n",
    "        \"ColumnList\": None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e928797-8899-4b00-836b-ccf66e360aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48bc369d-ce56-402d-84d5-d427d0fd5f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/MDFs/ADLS to Lakehouse/functions/NB_Functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbb1750-18e2-4289-ac91-73dbdfce289a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mappings = json.loads(mappings)\n",
    "    start_time = spark.sql(\"SELECT CURRENT_TIMESTAMP()\").first()[0]\n",
    "\n",
    "    file_source_abfss_path = f\"{mappings['BaseUrl']}/{mappings['RootFolder']}/\"\n",
    "\n",
    "    target_table_name = f\"{mappings['TargetCatalog']}.{mappings['TargetSchema']}.{mappings['TargetTable']}\"\n",
    "\n",
    "    autoloader_schema_location = f\"{mappings['BaseUrl']}/autoloader/{mappings['TargetCatalog']}_{mappings['TargetSchema']}_{mappings['TargetTable']}/schemalocation\"\n",
    "\n",
    "    autoloader_checkpoint_location = f\"{mappings['BaseUrl']}/autoloader/{mappings['TargetCatalog']}_{mappings['TargetSchema']}_{mappings['TargetTable']}/checkpointlocation\"\n",
    "\n",
    "    col_list = spark.sql(\n",
    "        f\"SELECT COALESCE(ColumnList, '') FROM {mappings['TargetCatalog']}.{mappings['TargetSchema']}.file_master_config WHERE TargetTable = '{mappings['TargetTable']}' AND EnableFlag=1\"\n",
    "    ).first()[0]\n",
    "    cols = col_list.replace('\"', \"\").replace(\" \", \"\").split(\",\")\n",
    "\n",
    "    if cols == [\"\"]:\n",
    "        cols = \"*\"\n",
    "    else:\n",
    "        cols = cols + [\"FilePath\", \"SourceFile\", \"InsertTimestamp\"]\n",
    "\n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", autoloader_schema_location)\n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"True\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"*.csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"delimiter\", \",\")\n",
    "        .option(\"quote\", '\"')\n",
    "        .option(\"escape\", '\"')\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"lineSep\", \"\\n\")\n",
    "        .load(file_source_abfss_path)\n",
    "    )\n",
    "\n",
    "    df = df.toDF(*[col.strip() for col in df.columns])\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"FilePath\", F.regexp_replace(F.input_file_name(), \"%20\", \" \"))\n",
    "        .withColumn(\"SourceFile\", F.regexp_extract(\"FilePath\", r\"([^/]+$)\", 1))\n",
    "        .withColumn(\"InsertTimestamp\", F.current_timestamp())\n",
    "        .withColumn(\n",
    "            \"Timestamp\",\n",
    "            F.to_timestamp(\n",
    "                F.regexp_replace(F.col(\"Timestamp\"), r\"[\\r\\n]+\", \"\"), \"yyyy-MM-dd HH:mm:ss\"\n",
    "            ),\n",
    "        )\n",
    "        .filter(F.col(mappings['WatermarkColumn']) > F.lit(mappings[\"LPWatermarkValue\"]))\n",
    "        .select(cols)\n",
    "    )\n",
    "\n",
    "    if spark.catalog.tableExists(target_table_name):\n",
    "        spark.sql(f\"TRUNCATE TABLE {target_table_name}\")\n",
    "\n",
    "    (\n",
    "        df.writeStream.format(\"delta\")\n",
    "        .option(\"checkpointLocation\", autoloader_checkpoint_location)\n",
    "        .trigger(once=True)\n",
    "        .toTable(target_table_name)\n",
    "    )\n",
    "\n",
    "    end_time = spark.sql(\"SELECT CURRENT_TIMESTAMP()\").first()[0]\n",
    "    log_audit(\n",
    "        job_name, job_run_id, 'Success', \"\", start_time, end_time\n",
    "    )\n",
    "except Exception as e:\n",
    "    if re.search(r\"UNABLE_TO_INFER_SCHEMA\", str(e)):\n",
    "        print(\"No files found after the specific cutoff time.\")\n",
    "    else:\n",
    "        end_time = spark.sql(\"SELECT CURRENT_TIMESTAMP()\").first()[0]\n",
    "        error_details = str(e)[:1000].replace(\"'\", \"\\\"\")\n",
    "        log_audit(\n",
    "            job_name, job_run_id, 'Fail', error_details, start_time, end_time\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0352bb24-2af9-42cf-ae68-21e1a7b67bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7627472833221793,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_SourceToBronze",
   "widgets": {
    "JobName": {
     "currentValue": "-1",
     "nuid": "59703739-f963-4227-b24b-ff2030764b5b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "JobName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "JobName",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "JobRunID": {
     "currentValue": "-1",
     "nuid": "b03a8149-ad27-448e-9ef6-fa0996f2688b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "JobRunID",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "JobRunID",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
