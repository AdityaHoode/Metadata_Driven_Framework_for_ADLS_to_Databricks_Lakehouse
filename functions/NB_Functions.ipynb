{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e0d2f8-4f19-4d96-8791-88335e5bc4f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T, functions as F, Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "import re\n",
    "from pyspark.errors import AnalysisException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d81f6c6-7ad0-4363-a99a-6a17b781de2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_audit(\n",
    "    job_name,\n",
    "    job_run_id,\n",
    "    status,\n",
    "    error_details=\"\",\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    audit_table=\"mdf2.bronze.audit\"\n",
    "):\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO\n",
    "        {audit_table}\n",
    "    VALUES\n",
    "    (\n",
    "        '{job_name}',\n",
    "        '{job_run_id}',\n",
    "        '{status}',\n",
    "        '{error_details}',\n",
    "        try_cast('{start_time}' AS TIMESTAMP),\n",
    "        try_cast('{end_time}' AS TIMESTAMP)\n",
    "    )\n",
    "    \"\"\"\n",
    "    spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca01346d-63e2-412c-963c-902c490d6344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_metadata_tables(catalog, schema):\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {catalog}.{schema}.file_master_config (\n",
    "            RootFolder STRING,\n",
    "            TargetCatalog STRING,\n",
    "            TargetSchema STRING,\n",
    "            TargetTable STRING,\n",
    "            FileSearchTerm STRING,\n",
    "            BaseUrl STRING,\n",
    "            WatermarkColumn STRING,\n",
    "            LPWatermarkValue TIMESTAMP,\n",
    "            EnableFlag BOOLEAN,\n",
    "            ColumnList STRING\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {catalog}.{schema}.audit (\n",
    "            JobName STRING,\n",
    "            JobRunID STRING,\n",
    "            Status STRING,\n",
    "            ErrorDetails STRING,\n",
    "            StartTime TIMESTAMP,\n",
    "            EndTime TIMESTAMP\n",
    "        );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc7773f-4c5a-4fab-8592-5c50e2605cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_file_master_config(\n",
    "    rootFolder: str,\n",
    "    targetCatalog: str,\n",
    "    targetSchema: str,\n",
    "    targetTable: str,\n",
    "    fileSearchTerm: str,\n",
    "    baseUrl: str,\n",
    "    watermarkColumn: str,\n",
    "    lpWatermarkValue: str,\n",
    "    enableFlag: str,\n",
    "    columnList: str,\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    ") -> dict:\n",
    "\n",
    "    schema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"RootFolder\", T.StringType(), True),\n",
    "            T.StructField(\"TargetCatalog\", T.StringType(), True),\n",
    "            T.StructField(\"TargetSchema\", T.StringType(), True),\n",
    "            T.StructField(\"TargetTable\", T.StringType(), True),\n",
    "            T.StructField(\"FileSearchTerm\", T.StringType(), True),\n",
    "            T.StructField(\"BaseUrl\", T.StringType(), True),\n",
    "            T.StructField(\"WatermarkColumn\", T.StringType(), True),\n",
    "            T.StructField(\"LPWatermarkValue\", T.TimestampType(), True),\n",
    "            T.StructField(\"EnableFlag\", T.BooleanType(), True),\n",
    "            T.StructField(\"ColumnList\", T.StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if watermarkColumn is not None:\n",
    "        watermarkColumn = str(re.sub(r\"[ /()&]\", \"\", watermarkColumn))\n",
    "        watermarkColumn = str(watermarkColumn.replace(\"-\", \"_\"))\n",
    "    else:\n",
    "        watermarkColumn = str(None)\n",
    "\n",
    "    lpWatermarkValue = datetime.strptime(lpWatermarkValue, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    data = [\n",
    "        Row(\n",
    "            rootFolder,\n",
    "            targetCatalog,\n",
    "            targetSchema,\n",
    "            targetTable,\n",
    "            fileSearchTerm,\n",
    "            baseUrl,\n",
    "            watermarkColumn,\n",
    "            lpWatermarkValue,\n",
    "            bool(enableFlag),\n",
    "            columnList,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    keyColumns = [\n",
    "        \"RootFolder\",\n",
    "        \"BaseUrl\",\n",
    "    ]\n",
    "\n",
    "    updateColumns = [\n",
    "        \"TargetCatalog\",\n",
    "        \"TargetSchema\",\n",
    "        \"TargetTable\",\n",
    "        \"FileSearchTerm\",\n",
    "        \"WatermarkColumn\",\n",
    "        \"LPWatermarkValue\",\n",
    "        \"EnableFlag\",\n",
    "        \"ColumnList\",\n",
    "    ]\n",
    "\n",
    "    valuesColumns = {\n",
    "        f\"target.{col}\": f\"source.{col}\" for col in keyColumns + updateColumns\n",
    "    }\n",
    "    \n",
    "    setColumns = {f\"target.{col}\": f\"source.{col}\" for col in updateColumns}\n",
    "\n",
    "    delta_table = DeltaTable.forName(\n",
    "        spark, f\"{catalog}.{schema}.file_master_config\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        delta_table.alias(\"target\")\n",
    "        .merge(\n",
    "            df.alias(\"source\"),\n",
    "            \" and \".join(f\"target.{c} = source.{c}\" for c in keyColumns),\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            \" or \".join([f\"target.`{c}` != source.`{c}`\" for c in updateColumns]),\n",
    "            set=setColumns,\n",
    "        )\n",
    "        .whenNotMatchedInsert(values=valuesColumns)\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    return get_record_stats(catalog, schema, \"file_master_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7903e137-86b5-4877-905e-6b5c8d26c9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
